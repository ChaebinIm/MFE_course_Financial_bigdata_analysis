{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import talib as ta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "#from factor_analyzer import FactorAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # 한 실행칸에 프린트 여러개 해도 다 출력시키도록 하는 코드.\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./data.csv does not exist: './data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ed7a4d3d7160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tech.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ./data.csv does not exist: './data.csv'"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('./data.csv')\n",
    "tech = pd.read_csv('./tech.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['datetime'] = raw_data['일자'] + ' ' + raw_data['시간']\n",
    "raw_data['datetime'] = pd.to_datetime(raw_data['datetime'], format='%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['datetime'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = raw_data.iloc[:65284]\n",
    "tech = tech.iloc[:65284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data[['datetime', '종가']]\n",
    "data.columns = ['datetime', 'close']\n",
    "data = pd.concat([data, tech], axis = 1)\n",
    "data = data.dropna(axis=0).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 120분 평균과 분산\n",
    "data['mean_120m'] = data['close'].rolling(120).mean()\n",
    "data['std_120m'] = data['close'].rolling(120).std()\n",
    "data.dropna(inplace = True)\n",
    "data.reset_index(inplace = True, drop = True)\n",
    "# y date list\n",
    "date_list = [1, 3, 5, 10, 15, 20]\n",
    "# making y\n",
    "for i in date_list:\n",
    "    close_after = data['close'].iloc[i:].reset_index(drop = True)\n",
    "    nans = np.empty(i)\n",
    "    nans = np.nan\n",
    "    close_after = close_after.append(pd.Series(nans, index = np.arange(len(close_after), len(close_after) + i)))\n",
    "    label =  'y_' + str(i) +'m'\n",
    "    lower_bound = data['close'] - data['std_120m'] / np.sqrt(120//i)\n",
    "    upper_bound = data['close'] + data['std_120m'] / np.sqrt(120//i)\n",
    "    print(data['std_120m'] / np.sqrt(120//i))\n",
    "    print(close_after)\n",
    "    print(upper_bound)\n",
    "    up = close_after >= upper_bound\n",
    "    down = close_after < lower_bound\n",
    "    y = 1 * up -1 * down\n",
    "    data[label] = y\n",
    "data.drop(['mean_120m', 'std_120m'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['y_1m'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data= data[['y_1m','y_3m','y_5m','y_10m','y_15m','y_20m']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(y_data.columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_selection_data_set = data[:-(396*40)]\n",
    "classify_predict_data_set = data[-(396*40):]\n",
    "\n",
    "train_test_index = classify_predict_data_set.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    해야 되는 것.\n",
    "    1. (1~20)분 후의 가격 ~ price_1m\n",
    "    2. (1~20)분 후의 로그수익률 ~ lgret_1m\n",
    "    3. (1~20)분 후의 절대수익률 ~ ret_1m\n",
    "\"\"\"\n",
    "data2 = pca_selection_data_set.copy()\n",
    "original_col = data2.columns.to_list()\n",
    "\n",
    "for i in range(1, 21):\n",
    "    num_label = str(i) + 'm'\n",
    "    data2['price_' + num_label] = (data2['close'].shift(-i) - data2['close']) * 100\n",
    "    data2['ret_' + num_label] = ((data2['close'].shift(-i) - data2['close'])/data2['close']) * 100\n",
    "    data2['logret_' + num_label] = (np.log(data2['close'].shift(-i)/data2['close'])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(method, period): # method : ('price', 'ret', 'logret'), period = 1~20\n",
    "    label = method + '_' + str(period) + 'm'\n",
    "    cols = original_col.copy()\n",
    "    cols.append(label)\n",
    "    _data = data2[cols]\n",
    "    # _data.set_index('datetime', inplace = True, drop = True)\n",
    "    _data.dropna(inplace = True)\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lasso_result(x, parameter):\n",
    "#     all_dict = {}\n",
    "\n",
    "#     X_train = x.iloc[:,:-1]\n",
    "#     y_train = x.iloc[:,[-1]]\n",
    "\n",
    "#     fig, axs = plt.subplots(figsize=(200,200), nrows=1, ncols=len(parameter) )\n",
    "#     coeff_df = pd.DataFrame()\n",
    "\n",
    "#      # sns.set(font=\"AppleGothic\", \n",
    "#      #        rc={\"axes.unicode_minus\":False},\n",
    "#       #       style='darkgrid',font_scale=15)    \n",
    "\n",
    "#     for pos, al in enumerate(parameter):\n",
    "#         lasso = Lasso(alpha=al)\n",
    "#         lasso.fit ( X_train, y_train)\n",
    "#         print(lasso)\n",
    "#         coeff = pd.DataFrame(data=lasso.coef_, index=list(X_train.columns),columns=['coeff_value'])\n",
    "#         coeff['abs_coeff_value'] = abs(coeff['coeff_value'])\n",
    "#         coeff = coeff.sort_values('abs_coeff_value',ascending=False)\n",
    "#         colname = 'alpha:'+str(al)\n",
    "#       #  axs[pos].set_title(colname)\n",
    "#        # axs[pos].set_xlim(-4,4)  \n",
    "        \n",
    "#         coeff_notzero= coeff['coeff_value'][coeff['abs_coeff_value']>=0.01]\n",
    "#       #  if len(coeff_notzero)>0:\n",
    "#            # sns.barplot( x=coeff_notzero.values, y=coeff_notzero.index, ax=axs[pos])\n",
    "#         all_dict[colname] = coeff\n",
    "#     return(all_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lasso_feature_selection(rank):  \n",
    "#     list_all = pd.Series()\n",
    "#     alpha_all = pd.Series()\n",
    "    \n",
    "#     alphas = [0.0001, 0.001, 0.01,0.1]\n",
    "#     for i in ['price', 'ret', 'logret'] :\n",
    "#         for j in [1,3,5,7,9,11,13,15,17,19]:\n",
    "#             result_lasso= lasso_result(extract_data(i,j).iloc[:-1],alphas)\n",
    "#             for k in range(len(list(result_lasso.keys()))):        \n",
    "#                 result_frame_temp = result_lasso[list(result_lasso.keys())[k]] \n",
    "#                 list_temp=pd.Series(result_frame_temp[result_frame_temp['abs_coeff_value']>0.01].iloc[:rank].index)\n",
    "#                 list_all=list_all.append(list_temp)\n",
    "#                 alpha_all = alpha_all.append(pd.Series(list(result_lasso.keys())[k]).repeat(len(list_temp)))\n",
    "#                 print(i)\n",
    "#                 print(j)\n",
    "#                 print(list_all.value_counts())\n",
    "                \n",
    "#                 print(result_frame_temp[result_frame_temp['abs_coeff_value']>0.01].iloc[:rank])\n",
    "#     alpha_all.reset_index(inplace=True, drop=True)\n",
    "#     list_all.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "#     final_frame = pd.DataFrame({'feature_all':list_all,'alpha':alpha_all})\n",
    "#     return(final_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_varaible = lasso_feature_selection(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######여기서부터 돌리기#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#선택된 칼럼들\n",
    "#selected_variable= list(result_varaible['feature_all'].value_counts().index)\n",
    "selected_variable = pd.read_csv('./lasso_selection_variable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variable = selected_variable['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run all\n",
    "# selected_variable = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name= 'lasso_selection_variable'\n",
    "#result_csv = open(\"./result_\"+name+\".csv\", \"a\", newline=\"\", encoding=\"utf8\")\n",
    "\n",
    "#with result_csv:\n",
    "#    write = csv.writer(result_csv)\n",
    "#    write.writerows([list(result_varaible['feature_all'].value_counts().index)])\n",
    "\n",
    "#tick_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(selected_variable).to_csv('./lasso_selection_variable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca_analysis(_train_set, _test_set, _LASSO_columns, threshold, draw_plot):\n",
    "    \n",
    "    train_pca_res = train_set_pca_factor(_train_set, threshold, _LASSO_columns, draw_plot)\n",
    "    \n",
    "    train_pca_factors = train_pca_res[0]\n",
    "    train_loadings = train_pca_res[1]\n",
    "    \n",
    "    LASSOed_test_set = _test_set[_LASSO_columns]\n",
    "    test_pca_factors = LASSOed_test_set @ train_loadings\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_pca_factors, train_loadings, test_pca_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = LASSO로 의미없는 변수 제거한 DataFrame\n",
    "\n",
    "def train_set_pca_factor(_train_set, threshold, _LASSO_columns, draw_plot = False):\n",
    "    \n",
    "    LASSOed_train_set = _train_set[_LASSO_columns]\n",
    "    max_pca_factor = LASSOed_train_set.shape[1]\n",
    "    LASSOed_train_set = StandardScaler().fit_transform(LASSOed_train_set)\n",
    "    pca = PCA(n_components = max_pca_factor)\n",
    "    principalComponents = pca.fit_transform(LASSOed_train_set)\n",
    "    explain_var = pca.explained_variance_ratio_\n",
    "    num_pca_factor = len(explain_var[explain_var.cumsum() < threshold])\n",
    "    print(\"PCA threshold 넘기기 위해 필요한 최소 Factor: \",num_pca_factor)\n",
    "    \n",
    "    if draw_plot == True:\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=1)\n",
    "\n",
    "        axes[0].plot(explain_var.cumsum(), label = \"explained_Var\")\n",
    "        axes[1].set_xlabel(\"# of PCA Factors\")\n",
    "        axes[0].set_ylabel(\"cum_explained_variance\")\n",
    "        axes[0].set_title(\"PCA_explained_variance\")\n",
    "        axes[0].axhline(y=0.7, color='r', linewidth=1, label = \"dd\")\n",
    "        axes[0].legend([\"explained_Var\", \"Threshold\"])\n",
    "\n",
    "\n",
    "        axes[1].bar(np.arange(max_pca_factor) , explain_var)\n",
    "        axes[1].set_ylabel(\"explained_variance\")\n",
    "    \n",
    "    pca = PCA(n_components = num_pca_factor)\n",
    "    principalComponents = pca.fit_transform(LASSOed_train_set)\n",
    "    train_set_pca_factors = pd.DataFrame(data=principalComponents, \n",
    "                                         columns=[\"PC\"+str(i+1) for i in range(num_pca_factor)])\n",
    "    \n",
    "    _LASSO_columns = np.array(_LASSO_columns)\n",
    "    train_set_loadings = pd.DataFrame(pca.components_.T,\n",
    "                        columns=[\"PC\"+str(i+1) for i in range(num_pca_factor)],\n",
    "                        index=np.array(_LASSO_columns).T)\n",
    "    \n",
    "    return train_set_pca_factors, train_set_loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 396 # 1 day data\n",
    "train_size = window_size * 5\n",
    "test_size = window_size\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "for j in range(len(date_list)):\n",
    "    y_label = \"y_\" + str(date_list[j]) + \"m\"\n",
    "    y = y_data.loc[train_test_index:, y_label]\n",
    "    i = train_test_index\n",
    "    test_results = pd.DataFrame(columns=['act_'+str(date_list[j]),'pred_'+str(date_list[j])])\n",
    "    while(i + train_size + test_size <= len(data)):\n",
    "        print(y_label,raw_data['datetime'].loc[i + train_size + test_size] )\n",
    "        print(raw_data['datetime'].loc[len(data)])\n",
    "        \n",
    "        # PCA func (with YK's func)\n",
    "        _, _, sub_data = run_pca_analysis(pca_selection_data_set, \n",
    "                                          classify_predict_data_set.loc[i:i+train_size + test_size] ,\n",
    "                                          selected_variable, 0.7, draw_plot = True )\n",
    "        \n",
    " \n",
    "           # 데이터 쪼개기 : train & test data\n",
    "        train = pd.concat([sub_data.loc[i : i + train_size], y.loc[i : i + train_size]], axis=1)\n",
    "        test = pd.concat([sub_data.loc[i+train_size : i+train_size + test_size], y.loc[i+train_size : i+train_size + test_size]], axis=1)\n",
    "        \n",
    "        # train : like logistic regression(with softmax)\n",
    "        \n",
    "        softmax_reg = LogisticRegression(multi_class=\"multinomial\", penalty='l2', C=10, random_state=42)\n",
    "        softmax_reg.fit(train.iloc[:,:-1], train.iloc[:, -1])\n",
    "        \n",
    "        \n",
    "        # test : \n",
    "        res = softmax_reg.predict(test.iloc[:,:-1])\n",
    "        \n",
    "        test_results = pd.concat([test_results, \n",
    "                                  pd.DataFrame({'act_'+str(date_list[j]):test.iloc[:, -1],\n",
    "                                                'pred_'+str(date_list[j]):res})])\n",
    "        \n",
    "        i = i + window_size\n",
    "        \n",
    "    result_dict[y_label] = test_results # {'y_1m' : [], 'y_3m' : []} []은 y & yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = result_dict['y_10m']['act_10'].to_list()\n",
    "pred = result_dict['y_10m']['pred_10'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "accuracy_metric = accuracy_score(act, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(act, pred, average=\"macro\"))\n",
    "print(precision_score(act, pred, average=\"macro\"))\n",
    "print(recall_score(act, pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(act, pred)\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(act, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading Profit Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##필요한 DataSet\n",
    "predict_res_dict= result_dict.copy()\n",
    "temp_d = classify_predict_data_set.copy()\n",
    "original_col = temp_d.columns.to_list()\n",
    "\n",
    "##Label 생성\n",
    "for i in range(1, 21):\n",
    "    num_label = str(i) + 'm'\n",
    "    \n",
    "    #price change times 100!\n",
    "    temp_d['price_' + num_label] = (temp_d['close'].shift(-i) - temp_d['close']) * 100\n",
    "    \n",
    "    #만원단위 Tick Value(Lead로 계산하기위해 shift(-i))\n",
    "    temp_d['tick_value_' + num_label] = (temp_d['close'].shift(-i) - temp_d['close'])/0.05*2.5\n",
    "    \n",
    "    temp_d['ret_' + num_label] = ((temp_d['close'].shift(-i) - temp_d['close'])/temp_d['close']) * 100\n",
    "    temp_d['logret_' + num_label] = (np.log(temp_d['close'].shift(-i)/temp_d['close'])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_profit_RESULT = pd.DataFrame()\n",
    "\n",
    "##검증 필요\n",
    "y = [1, 3, 5, 10, 15, 20]\n",
    "\n",
    "for i in y:\n",
    "    profit_df = pd.merge(temp_d[\"tick_value_\"+str(i)+\"m\"],result_dict[\"y_\"+str(i)+\"m\"][\"pred_\"+str(i)],\n",
    "                         left_index= True, right_index=True, how =\"left\")\n",
    "    #앞: 5일치 train 데이터 빠짐 \n",
    "    #뒤: lead 계산한 만큼 빠짐\n",
    "#     profit_df = profit_df.dropna()\n",
    "    \n",
    "    \n",
    "    for row in range(len(profit_df)):\n",
    "        if profit_df[\"pred_\"+str(i)].iloc[row] == 1 or profit_df[\"pred_\"+str(i)].iloc[row] == -1:\n",
    "            profit_df[\"pred_\"+str(i)].iloc[row+1:row+5] = 0\n",
    "            \n",
    "#         elif profit_df[\"pred_\"+str(i)].iloc[row] == -1:\n",
    "#             profit_df[\"pred_\"+str(i)].iloc[row+1:row+5] = 0\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    profit_df[\"profit\"] = profit_df[\"tick_value_\"+str(i)+\"m\"] * profit_df[\"pred_\"+str(i)]\n",
    "    trading_profit_RESULT[str(i)+\"m_cum_profit\"] = profit_df[\"profit\"].cumsum()\n",
    "    \n",
    "trading_profit_RESULT.dropna(how = \"all\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_profit_RESULT[\"1m_cum_profit\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_profit_RESULT[\"3m_cum_profit\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_profit_RESULT[\"5m_cum_profit\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_profit_RESULT[\"10m_cum_profit\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_profit_RESULT[\"15m_cum_profit\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_profit_RESULT[\"20m_cum_profit\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_profit_RESULT.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
